#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Rules ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ
- ì¤‘ë³µ Rules ìë™ ì œê±°
- ì˜¤ë˜ëœ ìë™ í•™ìŠµ Rules ì•„ì¹´ì´ë¸Œ
- Rules êµ¬ì¡° ìµœì í™”
- ì •ê¸° ì •ë¦¬ í”„ë¡œì„¸ìŠ¤
"""

import os
import sys
import json
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any
import re

WORKSPACE_ROOT = Path(__file__).parent.parent
RULES_DIR = WORKSPACE_ROOT / ".cursor" / "rules"
PATTERNS_DIR = WORKSPACE_ROOT / ".cursor" / "patterns"
ARCHIVE_DIR = WORKSPACE_ROOT / ".cursor" / "rules_archive"
ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)

# Rules ìµœì í™” ê¸°ì¤€
OPTIMAL_RULES_COUNT = 42  # ìµœì†Œ Rules íŒŒì¼ ìˆ˜
TARGET_RULES_COUNT = 6  # ìµœì  Layer êµ¬ì¡°
MAX_AUTO_LEARNED_AGE_DAYS = 30  # 30ì¼ ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•Šì€ ìë™ í•™ìŠµ Rules ì•„ì¹´ì´ë¸Œ


class RulesAutoCleanup:
    """Rules ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.rules_dir = RULES_DIR
        self.archive_dir = ARCHIVE_DIR
        self.cleanup_stats = {
            "duplicates_removed": 0,
            "old_rules_archived": 0,
            "total_rules_before": 0,
            "total_rules_after": 0,
            "removed_files": [],
            "archived_files": []
        }
    
    def cleanup_all(self, dry_run: bool = False) -> Dict[str, Any]:
        """ì „ì²´ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("="*70)
        print("ğŸ”„ Rules ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ")
        print("="*70)
        print(f"â° ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ëª¨ë“œ: {'DRY RUN (ì‹œë®¬ë ˆì´ì…˜)' if dry_run else 'ì‹¤ì œ ì‹¤í–‰'}")
        print()
        
        # í˜„ì¬ Rules íŒŒì¼ ìˆ˜
        all_rules = list(self.rules_dir.glob("*.mdc"))
        self.cleanup_stats["total_rules_before"] = len(all_rules)
        print(f"ğŸ“Š í˜„ì¬ Rules íŒŒì¼ ìˆ˜: {len(all_rules)}ê°œ")
        print()
        
        # 1. ì¤‘ë³µ Rules ì œê±°
        print("1ï¸âƒ£ ì¤‘ë³µ Rules ê°ì§€ ë° ì œê±° ì¤‘...")
        duplicates_removed = self.remove_duplicate_rules(dry_run)
        self.cleanup_stats["duplicates_removed"] = duplicates_removed
        print(f"   âœ… ì¤‘ë³µ ì œê±°: {duplicates_removed}ê°œ")
        print()
        
        # 2. ì˜¤ë˜ëœ ìë™ í•™ìŠµ Rules ì•„ì¹´ì´ë¸Œ
        print("2ï¸âƒ£ ì˜¤ë˜ëœ ìë™ í•™ìŠµ Rules ì•„ì¹´ì´ë¸Œ ì¤‘...")
        old_archived = self.archive_old_auto_learned(dry_run)
        self.cleanup_stats["old_rules_archived"] = old_archived
        print(f"   âœ… ì•„ì¹´ì´ë¸Œ: {old_archived}ê°œ")
        print()
        
        # 3. Rules êµ¬ì¡° ìµœì í™”
        print("3ï¸âƒ£ Rules êµ¬ì¡° ìµœì í™” ì¤‘...")
        optimized = self.optimize_rules_structure(dry_run)
        print(f"   âœ… ìµœì í™” ì™„ë£Œ")
        print()
        
        # ìµœì¢… ê²°ê³¼
        all_rules_after = list(self.rules_dir.glob("*.mdc"))
        self.cleanup_stats["total_rules_after"] = len(all_rules_after)
        
        print("="*70)
        print("ğŸ“Š ì •ë¦¬ ê²°ê³¼")
        print("="*70)
        print(f"ì •ë¦¬ ì „: {self.cleanup_stats['total_rules_before']}ê°œ")
        print(f"ì •ë¦¬ í›„: {self.cleanup_stats['total_rules_after']}ê°œ")
        print(f"ê°ì†Œ: {self.cleanup_stats['total_rules_before'] - self.cleanup_stats['total_rules_after']}ê°œ")
        print()
        
        if duplicates_removed > 0:
            print(f"âœ… ì¤‘ë³µ ì œê±°: {duplicates_removed}ê°œ")
        if old_archived > 0:
            print(f"âœ… ì•„ì¹´ì´ë¸Œ: {old_archived}ê°œ")
        
        print()
        
        if dry_run:
            print("âš ï¸ DRY RUN ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” ë³€ê²½ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            print("âœ… Rules ì •ë¦¬ ì™„ë£Œ!")
        
        return self.cleanup_stats
    
    def remove_duplicate_rules(self, dry_run: bool = False) -> int:
        """ì¤‘ë³µ Rules ì œê±°"""
        all_rules = list(self.rules_dir.glob("*.mdc"))
        removed_count = 0
        
        # íŒŒì¼ ë‚´ìš© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬
        rule_contents = {}
        for rule_file in all_rules:
            try:
                content = rule_file.read_text(encoding='utf-8')
                # ë©”íƒ€ë°ì´í„° ì œê±° í›„ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ
                core_content = self._extract_core_content(content)
                rule_contents[rule_file] = core_content
            except Exception as e:
                print(f"   âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {rule_file.name} - {e}")
                continue
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ê°ì§€
        processed = set()
        duplicate_groups = []
        
        for rule1, content1 in rule_contents.items():
            if rule1 in processed:
                continue
            
            group = [rule1]
            
            for rule2, content2 in rule_contents.items():
                if rule1 == rule2 or rule2 in processed:
                    continue
                
                # ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„)
                similarity = self._calculate_similarity(content1, content2)
                
                if similarity > 0.8:  # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                    group.append(rule2)
                    processed.add(rule2)
            
            if len(group) > 1:
                duplicate_groups.append(group)
                processed.add(rule1)
        
        # ì¤‘ë³µ ê·¸ë£¹ì—ì„œ í’ˆì§ˆì´ ë†’ì€ ê²ƒë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°
        for group in duplicate_groups:
            # ìš°ì„ ìˆœìœ„: priority ë‚®ì„ìˆ˜ë¡, íŒŒì¼ í¬ê¸° ì ì ˆí•œ ê²ƒ, ìµœê·¼ ìˆ˜ì •ëœ ê²ƒ
            best_rule = self._select_best_rule(group)
            others = [r for r in group if r != best_rule]
            
            for rule_file in others:
                if not dry_run:
                    # ë°±ì—… í›„ ì œê±°
                    backup_path = self.archive_dir / f"duplicate_{rule_file.name}"
                    shutil.copy2(rule_file, backup_path)
                    rule_file.unlink()
                    self.cleanup_stats["removed_files"].append(str(rule_file.relative_to(WORKSPACE_ROOT)))
                
                removed_count += 1
                print(f"   âŒ ì¤‘ë³µ ì œê±°: {rule_file.name} (ìœ ì§€: {best_rule.name})")
        
        return removed_count
    
    def archive_old_auto_learned(self, dry_run: bool = False) -> int:
        """ì˜¤ë˜ëœ ìë™ í•™ìŠµ Rules ì•„ì¹´ì´ë¸Œ"""
        auto_learned_rules = list(self.rules_dir.glob("*auto-learned*.mdc"))
        archived_count = 0
        cutoff_date = datetime.now() - timedelta(days=MAX_AUTO_LEARNED_AGE_DAYS)
        
        for rule_file in auto_learned_rules:
            try:
                # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
                mtime = datetime.fromtimestamp(rule_file.stat().st_mtime)
                
                if mtime < cutoff_date:
                    if not dry_run:
                        # ì•„ì¹´ì´ë¸Œë¡œ ì´ë™
                        archive_path = self.archive_dir / rule_file.name
                        shutil.move(str(rule_file), str(archive_path))
                        self.cleanup_stats["archived_files"].append(str(rule_file.relative_to(WORKSPACE_ROOT)))
                    
                    archived_count += 1
                    print(f"   ğŸ“¦ ì•„ì¹´ì´ë¸Œ: {rule_file.name} ({mtime.strftime('%Y-%m-%d')})")
            except Exception as e:
                print(f"   âš ï¸ ì•„ì¹´ì´ë¸Œ ì‹¤íŒ¨: {rule_file.name} - {e}")
        
        return archived_count
    
    def optimize_rules_structure(self, dry_run: bool = False) -> bool:
        """Rules êµ¬ì¡° ìµœì í™”"""
        # Layer êµ¬ì¡° ìœ ì§€ í™•ì¸
        layer1_rules = list(self.rules_dir.glob("layer1-*.mdc"))
        layer2_rules = list(self.rules_dir.glob("layer2-*.mdc"))
        
        print(f"   ğŸ“‹ Layer 1 Rules: {len(layer1_rules)}ê°œ")
        print(f"   ğŸ“‹ Layer 2 Rules: {len(layer2_rules)}ê°œ")
        
        # ìë™ ìƒì„± RulesëŠ” ë³„ë„ í´ë”ë¡œ ì´ë™ (ì„ íƒì )
        # í˜„ì¬ëŠ” ì•„ì¹´ì´ë¸Œë§Œ ìˆ˜í–‰
        
        return True
    
    def _extract_core_content(self, content: str) -> str:
        """Rules íŒŒì¼ì—ì„œ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ"""
        # í”„ë¡ íŠ¸ë§¤í„° ì œê±°
        if content.startswith("---"):
            parts = content.split("---", 2)
            if len(parts) >= 3:
                content = parts[2]
        
        # ì£¼ì„ ì œê±°
        lines = content.split('\n')
        core_lines = []
        for line in lines:
            # ì£¼ì„ ì œê±° (ë‹¨, ì¤‘ìš”í•œ ì„¹ì…˜ì€ ìœ ì§€)
            if line.strip().startswith('#') and 'í•µì‹¬' not in line and 'ì›ì¹™' not in line:
                continue
            core_lines.append(line)
        
        return '\n'.join(core_lines)
    
    def _calculate_similarity(self, content1: str, content2: str) -> float:
        """ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)"""
        # ë‹¨ì–´ ì§‘í•©ìœ¼ë¡œ ë³€í™˜
        words1 = set(re.findall(r'\w+', content1.lower()))
        words2 = set(re.findall(r'\w+', content2.lower()))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def _select_best_rule(self, rule_files: List[Path]) -> Path:
        """ì¤‘ë³µ ê·¸ë£¹ì—ì„œ ê°€ì¥ ì¢‹ì€ Rules ì„ íƒ"""
        best_rule = None
        best_score = -1
        
        for rule_file in rule_files:
            try:
                content = rule_file.read_text(encoding='utf-8')
                
                # ì ìˆ˜ ê³„ì‚°
                score = 0
                
                # 1. Priority ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (0ì´ ìµœê³ )
                priority_match = re.search(r'priority:\s*(\d+)', content)
                if priority_match:
                    priority = int(priority_match.group(1))
                    score += (10 - priority) * 10  # priority 0 = 100ì , 1 = 90ì , ...
                
                # 2. alwaysApply ìˆìœ¼ë©´ ê°€ì 
                if 'alwaysApply: true' in content:
                    score += 20
                
                # 3. íŒŒì¼ í¬ê¸° ì ì ˆ (500-2000 ë°”ì´íŠ¸)
                file_size = rule_file.stat().st_size
                if 500 <= file_size <= 2000:
                    score += 10
                elif file_size > 5000:  # ë„ˆë¬´ í¬ë©´ ê°ì 
                    score -= 10
                
                # 4. ìµœê·¼ ìˆ˜ì •ëœ ê²ƒ ê°€ì 
                mtime = datetime.fromtimestamp(rule_file.stat().st_mtime)
                days_old = (datetime.now() - mtime).days
                if days_old < 7:
                    score += 5
                
                if score > best_score:
                    best_score = score
                    best_rule = rule_file
            except Exception as e:
                print(f"   âš ï¸ Rules í‰ê°€ ì‹¤íŒ¨: {rule_file.name} - {e}")
                continue
        
        return best_rule or rule_files[0]  # ê¸°ë³¸ê°’: ì²« ë²ˆì§¸ íŒŒì¼


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Rules ìë™ ì •ë¦¬ ì‹œìŠ¤í…œ")
    parser.add_argument("--dry-run", action="store_true", help="ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ì‹¤ì œ ë³€ê²½ ì—†ìŒ)")
    parser.add_argument("--archive-only", action="store_true", help="ì•„ì¹´ì´ë¸Œë§Œ ì‹¤í–‰")
    parser.add_argument("--duplicates-only", action="store_true", help="ì¤‘ë³µ ì œê±°ë§Œ ì‹¤í–‰")
    
    args = parser.parse_args()
    
    cleanup = RulesAutoCleanup()
    
    if args.archive_only:
        result = cleanup.archive_old_auto_learned(dry_run=args.dry_run)
        print(f"âœ… ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: {result}ê°œ")
    elif args.duplicates_only:
        result = cleanup.remove_duplicate_rules(dry_run=args.dry_run)
        print(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {result}ê°œ")
    else:
        stats = cleanup.cleanup_all(dry_run=args.dry_run)
        
        # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        report_path = WORKSPACE_ROOT / "daily" / datetime.now().strftime("%Y-%m-%d") / f"rules_cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ë³´ê³ ì„œ ì €ì¥: {report_path}")


if __name__ == "__main__":
    main()

